{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19fba4e1-4ea9-4d41-b5fe-64da6cf45578",
   "metadata": {},
   "source": [
    "# Corrección del examen parcial 2 – Clasificación  \n",
    "**Nombre:** Camila García Ornelas  \n",
    "**Materia:** Laboratorio de Aprendizaje Estadístico  \n",
    "**Tema:** Correcciones Parcial 2\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Explica el modelo de regresión logística para clasificación. ¿Cómo se determina el umbral de decisión?\n",
    "\n",
    "**Respuesta corregida:**\n",
    "La regresión logística predice probabilidades entre 0 y 1 usando la función sigmoide o “logit”. Si el valor está por encima de un umbral (threshold), normalmente 0.5, se clasifica como una clase (por ejemplo, 1), y si está por debajo, como la otra (0). El umbral puede cambiarse dependiendo de la sensibilidad o precisión que se busque.\n",
    "\n",
    "**Error cometido:**  \n",
    "En la respuesta original solo se mencionó que “usa logit para clasificar probabilidades entre 0 y 1”, pero no se explicó el umbral de decisión, que era parte clave de la pregunta.\n",
    "\n",
    "**Cómo evitar el error:**  \n",
    "Cuando una pregunta tiene dos partes, leerla completa antes de escribir y verificar que cada parte esté respondida. Puedo hacer una lista rápida de subpreguntas para asegurar que nada se quede sin responder.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Explica la intuición de la máquina de soporte vectorial para clasificación. ¿Cómo se determina qué modelo es mejor? ¿Cuál es la mayor diferencia que tiene contra un modelo de regresión logística?\n",
    "\n",
    "**Respuesta corregida:**\n",
    "La SVM busca el hiperplano óptimo que separe las clases maximizando el margen entre los puntos más cercanos (vectores de soporte).  \n",
    "Se determina qué modelo es mejor comparando su rendimiento en validación o test (por ejemplo, usando precisión, recall o AUC).  \n",
    "La principal diferencia con la regresión logística es que la SVM no predice probabilidades, sino que se enfoca en separar las clases con el mayor margen posible.\n",
    "\n",
    "**Evaluación:**  \n",
    " **Contestada correctamente.**  \n",
    "Explica con precisión el concepto del kernel y el procedimiento.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ¿Cuáles son los componentes principales en un MLP para clasificación? Dibuja un ejemplo y señálalo.\n",
    "\n",
    "**Respuesta corregida:**\n",
    "Un MLP (Perceptrón Multicapa) es una red neuronal con capas de nodos conectados:  \n",
    "- **Capa de entrada:** recibe las variables.  \n",
    "- **Capas ocultas:** procesan la información con funciones de activación (como ReLU o sigmoid).  \n",
    "- **Capa de salida:** produce la clase final.  \n",
    "\n",
    "Aprende relaciones no lineales y generaliza mejor que una regresión logística.\n",
    "\n",
    "**Error cometido:**  \n",
    "La maestra marcó que faltó el dibujo, que era explícitamente solicitado.  \n",
    "\n",
    "**Cómo evitar el error:**  \n",
    "Cuando la instrucción diga “dibuja” o “señala”, incluir siempre un esquema, aunque sea simple, con flechas entre capas. Puedo dejar un espacio en el examen para agregarlo.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ¿Cuál es el procedimiento a seguir cuando los datos no son linealmente separables en una SVC?\n",
    "\n",
    "**Respuesta correcta (ya estaba bien):**\n",
    "Si los datos no son linealmente separables, se cambia el kernel a uno no lineal (como el polinomial, RBF o sigmoide), que permite proyectar los datos a un espacio de mayor dimensión donde sí puedan separarse.\n",
    "\n",
    "**Evaluación:**  \n",
    " **Contestada correctamente.**  \n",
    "Explica con precisión el concepto del kernel y el procedimiento.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Describe qué es un hiperparámetro. ¿Por qué es importante ajustarlos? Da dos ejemplos.\n",
    "\n",
    "**Respuesta corregida:**\n",
    "Un hiperparámetro es una configuración externa al modelo (no se aprende durante el entrenamiento) que controla su comportamiento.  \n",
    "Son importantes porque influyen directamente en el rendimiento y la capacidad de generalización.  \n",
    "**Ejemplos:** el `C` en una SVM (controla la penalización) y la `tasa de aprendizaje` en una red neuronal.\n",
    "\n",
    "**Evaluación:**  \n",
    " **Contestada correctamente.**  \n",
    "Explica con precisión el concepto del kernel y el procedimiento.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### 6. Dibuja un diagrama de flujo para describir el proceso de optimización bayesiana.\n",
    "\n",
    "**Respuesta corregida (con descripción textual, ya que no se dibujó):**\n",
    "\n",
    "Optimización Bayesiana busca la mejor opción posible mientras reduce la incertidumbre.  \n",
    "1. Se define una función objetivo (lo que se quiere optimizar).  \n",
    "2. Se elige un punto de partida y se evalúa.  \n",
    "3. Se usa el teorema de Bayes para actualizar creencias según los resultados.  \n",
    "4. Se modela la función con un proceso gaussiano (distribución normal).  \n",
    "5. Se busca el siguiente punto prometedor según intervalos de confianza.  \n",
    "6. Se repite el ciclo hasta encontrar el mejor valor (máximo o mínimo).\n",
    "\n",
    "**Error cometido:**  \n",
    "No se hizo el diagrama.  \n",
    "\n",
    "**Cómo evitar el error:**  \n",
    "Cuando una pregunta pida explícitamente un diagrama o gráfico, agregar uno, aunque sea esquemático, con flechas. Ejemplo: “Probar → Actualizar modelo → Elegir nuevos hiperparámetros → Repetir”.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. ¿Qué es la curva ROC y cómo se usa para evaluar el desempeño de un modelo?\n",
    "\n",
    "**Respuesta corregida:**\n",
    "La curva ROC compara la tasa de verdaderos positivos (TPR) con la tasa de falsos positivos (FPR) para distintos umbrales de decisión.  \n",
    "El área bajo la curva (AUC) mide el rendimiento total: mientras más grande sea el AUC, mejor distingue el modelo entre clases.\n",
    "\n",
    "**Error cometido:**  \n",
    "La respuesta original no mencionó que cada punto de la curva es un umbral diferente, que fue lo que el profesor señaló.\n",
    "\n",
    "**Cómo evitar el error:**  \n",
    "Recordar que la curva ROC se genera probando todos los umbrales posibles, así que al estudiar, siempre relacionar ROC = *umbrales variables + AUC*.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Describe un espacio de Hilbert.\n",
    "\n",
    "**Respuesta corregida:**\n",
    "Un espacio de Hilbert es un espacio vectorial infinito donde los vectores pueden ser funciones, y se define un producto interno que permite medir distancias y ángulos.  \n",
    "Es la base matemática para kernels en SVM.\n",
    "\n",
    "**Error cometido:**  \n",
    "La definición fue incompleta (“donde utilizamos los vectores como funciones”).  \n",
    "\n",
    "**Cómo evitar el error:**  \n",
    "Cuando explique un concepto teórico, incluir dos partes: qué es + para qué sirve. Así evito respuestas vagas.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. ¿Qué significa que una función de costo sea convexa? ¿Qué beneficios hay de que un modelo tenga una función de costo convexa?\n",
    "\n",
    "**Respuesta correcta (ya estaba bien):**\n",
    "Una función de costo convexa tiene **una sola solución óptima global**. Esto permite que los algoritmos de optimización **converjan correctamente** sin quedarse atrapados en mínimos locales.\n",
    "\n",
    "**Evaluación:**  \n",
    " **Contestada correctamente.**\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Piensa en los tres modelos aprendidos en este parcial: ¿en qué situaciones usarías cada uno y por qué?\n",
    "\n",
    "**Respuesta corregida:**\n",
    "- **Regresión logística:** cuando quiero clasificar y las relaciones son más o menos lineales, y necesito probabilidades interpretables.  \n",
    "- **SVC:** cuando los datos son más complejos o no lineales; útil con kernels.  \n",
    "- **MLP:** cuando los patrones son altamente no lineales o complejos (imágenes, voz, texto).\n",
    "\n",
    "**Error cometido:**  \n",
    "Se respondieron con métodos de búsqueda (GridSearch, RandomSearch, BayesianSearch), que no eran parte de esta pregunta.\n",
    "\n",
    "**Cómo evitar el error:**  \n",
    "Leer el contexto del examen (el tema era clasificación, no optimización). Antes de contestar, identificar a qué tema pertenece la pregunta.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8853da24-a9ae-46db-8c02-0c412d3d621b",
   "metadata": {},
   "source": [
    "# Corrección examen práctico.  \n",
    "\n",
    "**Pregunta original:**  \n",
    "> Utiliza optimización bayesiana para intentar encontrar el mínimo global de la siguiente función:  \n",
    "> \\( f(x, y, z) = (6x - 2)^2 \\sin(12x - 4) + (6y - 2)^2 \\cos(12y - 4) + (6z - 2)^2 \\sin(12z - 4) \\)  \n",
    "> donde las 3 variables están acotadas en [0, 1]. Crea 5 muestras iniciales e itera 15 veces para optimizar.  \n",
    "\n",
    "\n",
    "**Error cometido:**  \n",
    "El error fue:\n",
    "\n",
    "IndexError: index 2 is out of bounds for axis 1 with size 1\n",
    "\n",
    "Esto pasó porque generé la matriz de entrada con una sola columna: X = np.random.uniform(0, 1).reshape(-1, 1)... cuando la función objetivo f(X) esperaba 3 columnas (x, y, z), como en:x = X[:,0]; y = X[:,1]; z = X[:,2]. Además, me intimidé al ver tres variables, ya que los ejemplos vistos en clase eran unidimensionales. Intenté adaptar el código anterior sin detenerme a pensar en la forma que debía tener X.\n",
    "\n",
    "**Como evitar el error en el futuro:**  \n",
    "1. Verificar la dimensionalidad antes de usar reshape:Si hay 3 variables, usar (n, 3) en lugar de (-1, 1).\n",
    "2. Imprimir la forma de la matriz antes de entrenar:print(X.shape)\n",
    "3. Probar la función con un punto antes de iterar: f(np.array([[0.5, 0.5, 0.5]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33c2c655-5676-4c84-bc47-a3b59bc31acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 1 of parameter k2__length_scale is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 2 of parameter k2__length_scale is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 01 | x_next=[0.78387019 0.42288992 0.84398083], y_next=-6.932543, best_y=-6.932543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 1 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 02 | x_next=[0.8095589  0.09990413 0.99978836], y_next=9.581167, best_y=-6.932543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 1 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 03 | x_next=[0.8576582  0.40071594 0.79138537], y_next=-5.148039, best_y=-6.932543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 1 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 04 | x_next=[0.69855042 0.5566621  0.81127297], y_next=-10.434193, best_y=-10.434193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 05 | x_next=[0.8524372  0.69096485 0.83321313], y_next=-4.929892, best_y=-10.434193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 1 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 06 | x_next=[0.68682122 0.57927543 0.77384127], y_next=-12.014983, best_y=-12.014983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 1 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 07 | x_next=[0.4875302  0.87685249 0.68604743], y_next=7.182270, best_y=-12.014983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 1 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 08 | x_next=[0.721468   0.86669665 0.76591933], y_next=-1.223924, best_y=-12.014983\n",
      "Iter 09 | x_next=[0.63640649 0.47580171 0.7554192 ], y_next=-7.691702, best_y=-12.014983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10 | x_next=[0.66759207 0.61938676 0.75003019], y_next=-11.888420, best_y=-12.014983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11 | x_next=[0.04512084 0.25471767 0.03164135], y_next=2.571993, best_y=-12.014983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12 | x_next=[9.95014225e-01 3.16314995e-04 3.20869568e-01], y_next=13.081173, best_y=-12.014983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 13 | x_next=[0.44716485 0.98962677 0.00528996], y_next=2.888104, best_y=-12.014983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14 | x_next=[0.30383428 0.99921119 0.23821275], y_next=-2.479928, best_y=-12.014983\n",
      "Iter 15 | x_next=[0.95035514 0.60813707 0.76748541], y_next=3.694194, best_y=-12.014983\n",
      "x* = [0.68682122 0.57927543 0.77384127],  f(x*) = -12.014983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Optimización Bayesiana\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from scipy.stats import norm\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# ----------- Función objetivo -----------\n",
    "def f(X):\n",
    "    x = X[:, 0]\n",
    "    y = X[:, 1]\n",
    "    z = X[:, 2]\n",
    "    return ((6 * x - 2)**2) * np.sin(12 * x - 4) + \\\n",
    "           ((6 * y - 2)**2) * np.cos(12 * y - 4) + \\\n",
    "           ((6 * z - 2)**2) * np.sin(12 * z - 4)\n",
    "\n",
    "# ----------- Muestras iniciales -----------\n",
    "n_init = 5\n",
    "X = rng.uniform(0.0, 1.0, size=(n_init, 3))\n",
    "y = f(X)\n",
    "\n",
    "# ----------- GP (modelo sustituto) -----------\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=np.ones(3),\n",
    "                                   length_scale_bounds=(1e-2, 1e2))\n",
    "gp = GaussianProcessRegressor(kernel=kernel,\n",
    "                              normalize_y=True,\n",
    "                              n_restarts_optimizer=5,\n",
    "                              random_state=42)\n",
    "\n",
    "# ----------- Expected Improvement (minimización) -----------\n",
    "def expected_improvement(X_cand, gp, y_best, xi=0.01):\n",
    "    mu, sigma = gp.predict(X_cand, return_std=True)\n",
    "    sigma = sigma.reshape(-1, 1)\n",
    "    mu = mu.reshape(-1, 1)\n",
    "    with np.errstate(divide='warn'):\n",
    "        imp = y_best - mu - xi\n",
    "        Z = imp / sigma\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "    return ei.ravel()\n",
    "\n",
    "# ----------- Ciclo de optimización bayesiana -----------\n",
    "n_iter = 15\n",
    "n_candidates = 20000  # número de puntos a muestrear por iteración\n",
    "\n",
    "for it in range(1, n_iter + 1):\n",
    "    gp.fit(X, y)\n",
    "    X_cand = rng.uniform(0.0, 1.0, size=(n_candidates, 3))\n",
    "    y_best = np.min(y)\n",
    "    ei = expected_improvement(X_cand, gp, y_best, xi=0.01)\n",
    "    x_next = X_cand[np.argmax(ei)].reshape(1, -1)\n",
    "    y_next = f(x_next).ravel()\n",
    "    X = np.vstack([X, x_next])\n",
    "    y = np.hstack([y, y_next])\n",
    "    print(f\"Iter {it:02d} | x_next={x_next.flatten()}, y_next={y_next[0]:.6f}, best_y={np.min(y):.6f}\")\n",
    "\n",
    "# ----------- Resultado final -----------\n",
    "best_idx = np.argmin(y)\n",
    "x_star = X[best_idx]\n",
    "y_star = y[best_idx]\n",
    "print(f\"x* = {x_star},  f(x*) = {y_star:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f329df1-d4af-4f1f-933f-3df9f33bcb11",
   "metadata": {},
   "source": [
    "# Pregunta 2 — Regresión Logística (adidas.csv)\n",
    "\n",
    "**Pregunta original:**  \n",
    "> El dataset adidas contiene información acerca de productos y la calificación que se les dio por usuarios.  \n",
    "> Utiliza regresión logística para conseguir un modelo que nos ayude a decidir si un producto será clasificado como bueno (average_rating >= 4.3).  \n",
    "> Encuentra la significancia estadística de los factores utilizados.  \n",
    "> Cross-validation a utilizar: Train-test.  \n",
    "> Métrico: AUC. Explica el significado del score que arroje tu modelo.\n",
    "\n",
    "---\n",
    "\n",
    "**Error cometido:** \n",
    "El error fue que la respuesta quedó sin ejecutar ni entregar, y en el notebook se produjo un AttributeError:\n",
    "\n",
    "python\n",
    "y = (data[\"average_rating\"] >= 4.3.astype(int))\n",
    "  'float' object has no attribute 'astype'\n",
    "Esto ocurrió porque intenté aplicar .astype(int) a un número flotante (4.3) en lugar de al resultado booleano de la comparación.\n",
    "Además, por haber estado concentrada resolviendo la primera pregunta de optimización bayesiana, dejé esta parte incompleta y sin validar.\n",
    "\n",
    "**Como evitar el error en el futuro:**  \n",
    "1. Usar checklist antes de modelar:\n",
    "2. Confirmar nombre y tipo de columna del target.\n",
    "3. Revisar data.info() y data.head().\n",
    "4. Probar expresiones paso a paso antes de meterlas en una sola línea. Ejemplo:\n",
    "   cond = data[\"average_rating\"] >= 4.3\n",
    "   y = cond.astype(int)\n",
    "5. Dividir el tiempo del examen con una alarma por pregunta (no dejar una sin entregar).\n",
    "6. Usar plantillas prehechas para clasificación binaria con Logistic Regression y AUC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8169952e-ad4e-4b85-ae0e-f08c8ad18f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (test) = 0.7678\n"
     ]
    }
   ],
   "source": [
    "# 1) AUC con train-test\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Cargar\n",
    "data = pd.read_csv(\"adidas.csv\")\n",
    "\n",
    "# Target binario: \"bueno\" si average_rating >= 4.3\n",
    "data[\"average_rating\"] = pd.to_numeric(data[\"average_rating\"], errors=\"coerce\")\n",
    "y = (data[\"average_rating\"] >= 4.3).astype(int)\n",
    "\n",
    "# Evitar fuga de información y columnas problemáticas\n",
    "drop_cols = [\"average_rating\", \"url\", \"name\", \"sku\", \"source_website\"]\n",
    "X = data.drop(columns=[c for c in drop_cols if c in data.columns])\n",
    "\n",
    "# Separar tipos\n",
    "num_cols = X.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(exclude=[\"number\"]).columns.tolist()\n",
    "\n",
    "# Train-test split (lo que pide el enunciado)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Preprocesamiento + Regresión Logística\n",
    "pre = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", StandardScaler(with_mean=False), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[\n",
    "        (\"pre\", pre),\n",
    "        (\"logreg\", LogisticRegression(max_iter=2000, solver=\"lbfgs\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# AUC en test (métrica requerida)\n",
    "proba_test = clf.predict_proba(X_test)[:, 1]\n",
    "auc_test = roc_auc_score(y_test, proba_test)\n",
    "print(f\"AUC (test) = {auc_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58bbb6e-bc6f-49a3-887f-c945610c6ca8",
   "metadata": {},
   "source": [
    "El modelo logra un desempeño aceptable (AUC ≈ 0.77), lo que demuestra que puede diferenciar razonablemente bien entre productos con alta y baja calificación. Si se eligen dos productos al azar,uno bueno y uno no bueno, el modelo asignará una probabilidad más alta al producto bueno aproximadamente el 77% de las veces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f732da3e-de70-41d7-abda-4b680d3862d6",
   "metadata": {},
   "source": [
    "# Pregunta 3 — Clasificación (diabetes.csv)\n",
    "\n",
    "**Pregunta original:**  \n",
    "> Utiliza el modelo de tu elección, con los hiperparámetros y factores de tu elección para obtener el mejor predictor que puedas conseguir para decidir si un paciente del dataset padece diabetes.  \n",
    "> Cross-validation a utilizar: **K-Folds.**  \n",
    "> Métrico a utilizar: **Precisión.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Error cometido:**  \n",
    "El error principal fue un KeyError: 'diagnosis', porque intenté acceder a una columna que no existía en el DataFrame.  \n",
    "Esto ocurrió porque copié parte del código anterior (de otro dataset) sin confirmar los nombres reales de las columnas del dataset de diabetes. Además, por estar apresurada intentando resolver la primera pregunta de optimización bayesiana, no verifiqué el contenido del dataset con `data.head()` y `data.info()` antes de definir las variables X e y.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cómo evitar el error en el futuro:**  \n",
    "1. Revisar siempre las columnas disponibles con `data.columns` antes de definir el target (`y`).  \n",
    "2. Asegurarme de usar el dataset correcto para cada pregunta (evitar confundirlo con el de Adidas).  \n",
    "3. No copiar bloques de código sin adaptarlos al nuevo dataset. \n",
    "4. Comprobar los nombres exactos de las variables objetivo (por ejemplo: `\"Outcome\"`, `\"Class\"`, `\"Diabetes\"`, etc.).  \n",
    "5. Probar los pasos individualmente (primero cargar datos, luego verificar tipos, después modelar).  \n",
    "6. Dividir el tiempo del examen y dejar una celda para validación rápida antes de enviar.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0fe5d1e-96fb-448f-8745-16aad93e4085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n",
      "None\n",
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            6      148             72             35        0  33.6   \n",
      "1            1       85             66             29        0  26.6   \n",
      "2            8      183             64              0        0  23.3   \n",
      "3            1       89             66             23       94  28.1   \n",
      "4            0      137             40             35      168  43.1   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                     0.627   50        1  \n",
      "1                     0.351   31        0  \n",
      "2                     0.672   32        1  \n",
      "3                     0.167   21        0  \n",
      "4                     2.288   33        1  \n",
      "Precisión media (K-Folds): 0.7174\n"
     ]
    }
   ],
   "source": [
    "# PREGUNTA 3 — CLASIFICACIÓN (diabetes.csv)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import make_scorer, precision_score\n",
    "\n",
    "# 1. Cargar dataset (ajustar ruta si es necesario)\n",
    "data = pd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "# 2. Revisar estructura\n",
    "print(data.info())\n",
    "print(data.head())\n",
    "\n",
    "# 3. Definir variables (basado en el dataset estándar de Kaggle)\n",
    "# La variable objetivo en este dataset suele ser 'Outcome' (1 = diabético, 0 = no diabético)\n",
    "X = data.drop(columns=[\"Outcome\"])\n",
    "y = data[\"Outcome\"]\n",
    "\n",
    "# 4. Crear modelo y pipeline\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(max_iter=2000, solver=\"lbfgs\")\n",
    ")\n",
    "\n",
    "# 5. Validación cruzada (K-Folds)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 6. Evaluar precisión (métrica solicitada)\n",
    "scores = cross_val_score(\n",
    "    model, X, y, cv=kfold, scoring=make_scorer(precision_score)\n",
    ")\n",
    "\n",
    "print(f\"Precisión media (K-Folds): {scores.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adb9b6f-78a1-4853-8ea5-a5d962f57dc5",
   "metadata": {},
   "source": [
    "La precisión mide el porcentaje de casos predichos como “diabéticos” que realmente lo son.\n",
    "\n",
    "Si la precisión promedio fue 0.71, significa que el 71 % de los pacientes clasificados por el modelo como “diabéticos” lo son en realidad.\n",
    "\n",
    "Esta métrica es útil cuando el costo de un falso positivo (diagnosticar erróneamente diabetes) es relevante, aunque debe complementarse con recall para evitar omitir pacientes que sí la padecen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a74692-201a-462c-860f-6733590ff15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
